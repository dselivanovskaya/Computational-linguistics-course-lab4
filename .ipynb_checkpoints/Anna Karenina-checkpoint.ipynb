{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anna Karenina example\n",
    "(https://habr.com/ru/post/342738/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\katya\\anaconda3\\lib\\site-packages (1.22.2)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: jupyter in c:\\users\\katya\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-win_amd64.whl (3.4 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-win_amd64.whl (895 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\katya\\anaconda3\\lib\\site-packages (from tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter) (7.5.1)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter) (4.7.5)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter) (5.6.1)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter) (6.1.0)\n",
      "Requirement already satisfied: notebook in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter) (6.0.3)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter) (5.3.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.24.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (4.3.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (5.0.7)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in c:\\users\\katya\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (7.16.1)\n",
      "Requirement already satisfied: jupyter-client>=4.1 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from qtconsole->jupyter) (6.1.6)\n",
      "Requirement already satisfied: pygments in c:\\users\\katya\\anaconda3\\lib\\site-packages (from qtconsole->jupyter) (2.6.1)\n",
      "Requirement already satisfied: qtpy in c:\\users\\katya\\anaconda3\\lib\\site-packages (from qtconsole->jupyter) (1.9.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\katya\\anaconda3\\lib\\site-packages (from qtconsole->jupyter) (4.6.3)\n",
      "Requirement already satisfied: pyzmq>=17.1 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from qtconsole->jupyter) (19.0.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\katya\\anaconda3\\lib\\site-packages (from qtconsole->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (2.11.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: testpath in c:\\users\\katya\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.4.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\katya\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (3.1.5)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\katya\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (1.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter) (3.0.5)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from notebook->jupyter) (6.0.4)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\katya\\anaconda3\\lib\\site-packages (from notebook->jupyter) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from notebook->jupyter) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\katya\\anaconda3\\lib\\site-packages (from notebook->jupyter) (0.8.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\users\\katya\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\katya\\anaconda3\\lib\\site-packages (from traitlets>=4.3.1->ipywidgets->jupyter) (4.4.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter) (3.2.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\katya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.4.3)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.17.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\katya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\katya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter-client>=4.1->qtconsole->jupyter) (2.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jupyter-core->qtconsole->jupyter) (227)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jinja2>=2.4->nbconvert->jupyter) (1.1.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\katya\\anaconda3\\lib\\site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\katya\\anaconda3\\lib\\site-packages (from bleach->nbconvert->jupyter) (21.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\katya\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter) (0.16.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter) (21.2.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.7.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\katya\\anaconda3\\lib\\site-packages (from packaging->bleach->nbconvert->jupyter) (2.4.7)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=5f5ef280f2656607f26ed263adeef938d95854b64827dd2a4abd8c5dce9c3a61\n",
      "  Stored in directory: c:\\users\\katya\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: flatbuffers, tensorflow-io-gcs-filesystem, termcolor, grpcio, tensorboard-plugin-wit, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, tensorboard-data-server, protobuf, markdown, tensorboard, opt-einsum, gast, astunparse, libclang, google-pasta, keras-preprocessing, keras, tf-estimator-nightly, tensorflow, urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 urllib3-1.25.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: conda 4.10.1 requires ruamel_yaml_conda>=0.11.14, which is not installed.\n",
      "ERROR: selenium 4.1.0 has requirement urllib3[secure]~=1.26, but you'll have urllib3 1.25.11 which is incompatible.\n",
      "ERROR: json2xml 3.9.0 has requirement certifi==2021.5.30, but you'll have certifi 2020.6.20 which is incompatible.\n",
      "ERROR: json2xml 3.9.0 has requirement idna==3.2, but you'll have idna 2.10 which is incompatible.\n",
      "ERROR: json2xml 3.9.0 has requirement requests==2.26.0, but you'll have requests 2.24.0 which is incompatible.\n",
      "ERROR: json2xml 3.9.0 has requirement urllib3==1.26.7, but you'll have urllib3 1.25.11 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\katya\\anaconda3\\lib\\site-packages (1.22.2)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 430, in check_if_exists\n",
      "    self.satisfied_by = pkg_resources.get_distribution(str(no_marker))\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 481, in get_distribution\n",
      "    dist = get_provider(dist)\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 357, in get_provider\n",
      "    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 900, in require\n",
      "    needed = self.resolve(parse_requirements(requirements))\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 791, in resolve\n",
      "    raise VersionConflict(dist, req).with_context(dependent_req)\n",
      "pip._vendor.pkg_resources.ContextualVersionConflict: (urllib3 1.26.8 (c:\\users\\katya\\anaconda3\\lib\\site-packages), Requirement.parse('urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1'), {'requests'})\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 188, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 185, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 332, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 179, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 362, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 325, in _get_abstract_dist_for\n",
      "    req.check_if_exists(self.use_user_site)\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 434, in check_if_exists\n",
      "    existing_dist = pkg_resources.get_distribution(\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 481, in get_distribution\n",
      "    dist = get_provider(dist)\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 357, in get_provider\n",
      "    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 900, in require\n",
      "    needed = self.resolve(parse_requirements(requirements))\n",
      "  File \"C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 791, in resolve\n",
      "    raise VersionConflict(dist, req).with_context(dependent_req)\n",
      "pip._vendor.pkg_resources.ContextualVersionConflict: (urllib3 1.26.8 (c:\\users\\katya\\anaconda3\\lib\\site-packages), Requirement.parse('urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1'), {'requests'})\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем словарь символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 70, 111, 111,  98,   2,  79,  98, 114, 103, 111, 106, 111,  98,\n",
       "         1,  80, 103, 100,   2,  82, 106, 108, 112, 109,  98, 103, 100,\n",
       "       106, 121,   2,  87, 112, 109, 115, 116, 112, 107,   1,   1,   1,\n",
       "        92,  98, 115, 116, 126,   2, 113, 103, 114, 100,  98])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Создаем генератор, который возвращает пакеты размером\n",
    "       n_seqs x n_steps из массива arr.\n",
    "       \n",
    "       Аргументы\n",
    "       ---------\n",
    "       arr: Массив, из которого получаем пакеты\n",
    "       n_seqs: Batch size, количество последовательностей в пакете\n",
    "       n_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "    '''\n",
    "    # Считаем количество символов на пакет и количество пакетов, которое можем сформировать\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "\n",
    "    \n",
    "    # Сохраняем в массиве только символы, которые позволяют сформировать целое число пакетов\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Делаем reshape 1D -> 2D, используя n_seqs как число строк, как на картинке\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # пакет данных, который будет подаваться на вход сети\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # целевой пакет, с которым будем сравнивать предсказание, получаем сдвиганием \"x\" на один символ вперед\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 70 111 111  98   2]\n",
      " [126   6   2 121 116]\n",
      " [100 106 116   2 116]\n",
      " [ 98 123 106 119   2]\n",
      " [103 116 106 109 106]]\n",
      "\n",
      "y\n",
      " [[111 111  98   2  79]\n",
      " [  6   2 121 116 112]\n",
      " [106 116   2 116 103]\n",
      " [123 106 119   2 106]\n",
      " [116 106 109 106   6]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:5, :5])\n",
    "print('\\ny\\n', y[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "начнем строить модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Определяем placeholder'ы для входных, целевых данных, а также вероятности drop out\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        batch_size: Batch size, количество последовательностей в пакете\n",
    "        num_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "        \n",
    "    '''\n",
    "    # Объявляем placeholder'ы\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Placeholder для вероятности drop out\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Строим LSTM ячейку.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        keep_prob: Скаляр (tf.placeholder) для dropout keep probability\n",
    "        lstm_size: Размер скрытых слоев в LSTM ячейках\n",
    "        num_layers: Количество LSTM слоев\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Строим LSTM ячейку\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Начинаем с базовой LSTM ячейки\n",
    "        lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Добавляем dropout к ячейке\n",
    "        drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Стэкируем несколько LSTM слоев для придания глубины нашему deep learning\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    # Инициализируем начальное состояние LTSM ячейки\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Строим softmax слой и возвращаем результат его работы.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        \n",
    "        x: Входящий от LSTM тензор\n",
    "        in_size: Размер входящего тензора, (кол-во LSTM юнитов скрытого слоя)\n",
    "        out_size: Размер softmax слоя (объем словаря)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # вытягиваем и решэйпим тензор, выполняя преобразование 3D -> 2D\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Соединяем результат LTSM слоев с softmax слоем\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Считаем logit-функцию\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    # Используем функцию softmax для получения предсказания\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Считаем функцию потери на основании значений logit-функции и целевых значений.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        logits: значение logit-функции\n",
    "        targets: целевые значения, с которыми сравниваем предсказания\n",
    "        lstm_size: Количество юнитов в LSTM слое\n",
    "        num_classes: Количество классов в целевых значениях (размер словаря)\n",
    "        \n",
    "    '''\n",
    "    # Делаем one-hot кодирование целевых значений и решейпим по образу и подобию logits\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Считаем значение функции потери softmax cross entropy loss и возвращаем среднее значение\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Считаем функцию потери на основании значений logit-функции и целевых значений.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        logits: значение logit-функции\n",
    "        targets: целевые значения, с которыми сравниваем предсказания\n",
    "        lstm_size: Количество юнитов в LSTM слое\n",
    "        num_classes: Количество классов в целевых значениях (размер словаря)\n",
    "        \n",
    "    '''\n",
    "    # Делаем one-hot кодирование целевых значений и решейпим по образу и подобию logits\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Считаем значение функции потери softmax cross entropy loss и возвращаем среднее значение\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # Мы будем использовать эту же сеть для сэмплирования (генерации текста),\n",
    "        # при этом будем подавать по одному символу за один раз\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Получаем input placeholder'ы\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Строим LSTM ячейку\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Прогоняем данные через RNN слои\n",
    "        # Делаем one-hot кодирование входящих данных\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Прогоняем данные через RNN и собираем результаты\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Получаем предсказания (softmax) и результат logit-функции\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Считаем потери и оптимизируем (с обрезкой градиента)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100        # Размер пакета\n",
    "num_steps = 100         # Шагов в пакете\n",
    "lstm_size = 512         # Количество LSTM юнитов в скрытом слое\n",
    "num_layers = 2          # Количество LSTM слоев\n",
    "learning_rate = 0.001   # Скорость обучения\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-48825a83a7b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msave_every_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n\u001b[0m\u001b[0;32m      6\u001b[0m                 \u001b[0mlstm_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 learning_rate=learning_rate)\n",
      "\u001b[1;32m<ipython-input-97-54e7f495548d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, grad_clip, sampling)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Строим LSTM ячейку\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m### Прогоняем данные через RNN слои\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-f9646971ae0e>\u001b[0m in \u001b[0;36mbuild_lstm\u001b[1;34m(lstm_size, num_layers, batch_size, keep_prob)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Стэкируем несколько LSTM слоев для придания глубины нашему deep learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbuild_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Инициализируем начальное состояние LTSM ячейки\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-f9646971ae0e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Стэкируем несколько LSTM слоев для придания глубины нашему deep learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbuild_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Инициализируем начальное состояние LTSM ячейки\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-f9646971ae0e>\u001b[0m in \u001b[0;36mbuild_cell\u001b[1;34m(lstm_size, keep_prob)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Начинаем с базовой LSTM ячейки\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Добавляем dropout к ячейке\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Сохраняться каждый N итераций\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Можно раскомментировать строчку ниже и продолжить обучение с checkpoint'а\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Обучаем сеть\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
